\title{Probabilistic PCA}

\subsection{Probabilistic PCA}

Probabilistic PCA is useful when dealing with data that is dependent on both principal axes and latent variables \citep{Tipping99probabilisticprincipal}.

Consider a dataset $X = \{(\mathbf{x}_1, \mathbf{x}_2,\ldots , \mathbf{x}_n)\}$ where $\mathbf{x}_i \in \mathbbf{R}^D$.

Let this data be dependent both on the principal axes $\mathbf{w}_i$ as well as latent variables $\mathbf{z}_i$ for every $\mathbf{x}_i$.

Thus, we have that $$\mathbf{z} \sim N(0, \mathbf{I})$$ and that $$\mathbf{x} \vert \mathbf{z} \sim N(Wz + \mu, \sigma^2\mathbf{I})$$ where $\mathbf{\epsilon} \sim N(0, \sigma^2\mathbf{I})$.

Because of the dependence on $\mathbf{Wx}$, we don't want to solely want to infer the original principal axes (as would be done in the vanilla PCA model). Rather, we aim to model the principal axes with respect to the latent variables.

We set up our model below.

\begin{lstlisting}[language=Python]

w = Normal(mu=tf.zeros([D, K]), sigma=2.0 * tf.ones([D, K]))
z = Normal(mu=tf.zeros([N, K]), sigma=tf.ones([N, K]))
x = Normal(mu=tf.matmul(w, z, transpose_b=True), sigma=tf.ones([D, N]))

\end{lstlisting}

Since $\mathbf{W}$ cannot be analytically determined, we must use some approximation method. For this example, we use the KL $q\vert\vert p$ divergence. Below, we set up our inference variables and then run the algorithm.

\begin{lstlisting}[language=Python]


qw = Normal(mu=tf.Variable(tf.random_normal([D, K])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([D, K]))))
qz = Normal(mu=tf.Variable(tf.random_normal([N, K])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([N, K]))))

inference = ed.KLqp({w: qw, z: qz}, data={x: x_train})
\end{lstlisting}

\subsubsection{References}\label{references}
