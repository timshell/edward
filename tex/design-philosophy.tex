\title{Design Philosophy}

\subsection{Design Philosophy}

\textbf{Edward} serves two purposes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  to serve as a foundation for research;
\item
  to serve as a unified library for modeling, inference, and criticism.
\end{enumerate}

As a research tool, the code base provides a testbed for fast
experimentation. For example, to evaluate new inference algorithms,
the experiments only need to be written once and we can simply swap
the inference with the newly proposed algorithm. This idea applies to
all components of probabilistic modeling: we can leverage the built-in
inference algorithms to develop new complex models, or use the
built-in models and inference to develop new criticism techniques.

As an applied tool, Edward supports a wide variety of settings,
ranging from classical hierarchical models on small data sets to
complex deep probabilistic models on large data sets. With
\href{https://www.tensorflow.org}{TensorFlow} as a backend, Edward can
leverage features such as computational graphs, distributed training,
and CPU/GPU integration to deploy probabilistic modeling at scale.

\subsection{Related Software}\label{related-software}

There are several notable themes in Edward.

\textbf{Probabilistic programming.}
There has been much work on programming languages which specify
broad classes of probabilistic models, or probabilistic
programs. Recent works include
\href{http://projects.csail.mit.edu/church/wiki/Church}{Church}
\citep{goodman2012church},
\href{http://probcomp.csail.mit.edu/venture/}{Venture}
\citep{mansinghka2014venture},
\href{http://www.robots.ox.ac.uk/~fwood/anglican/literature/index.html}{Anglican}
\citep{wood2014new},
\href{http://mc-stan.org}{Stan}
\citep{carpenter2016stan}, and
\href{http://webppl.org}{WebPPL}
\citep{goodman2014design}.
The most important distinction in Edward comes from our motivation.
We are interested in deploying probabilistic models to a myriad of
real world applications, ranging from the size of data and
data structure, such as large text corpora or many brief audio signals,
to the size of model and class of models, such as small nonparametric
models or deep generative models.
Thus Edward is built with efficient (and possibly distributed)
training in mind, beyond the setting where all computing is on CPUs
or where data must fit in memory.

\textbf{Black box inference.}
Black box algorithms are typically based on Monte Carlo methods, and
by design make very few assumptions about the
model \citep{metropolis1949monte,hastings1970monte}.
Our motivation as outlined above presents a new set of
challenges in both inference research and their software design.
As one consequence, we focus on variational
inference \citep{hinton1993keeping,waterhouse1996bayesian,jordan1999introduction}.
As a second consequence, we encourage active research
on inference by providing a class hierarchy of inference algorithms.
As a third consequence, our inference
algorithms aim to take advantage of as much structure as possible from
the model. Edward supports all types of inference, whether they
be black box or model specific \citep{dempster1977maximum,hoffman2013stochastic}.

\textbf{Computational frameworks.}
There are many computational frameworks, primarily built for deep
learning: as of this date, this includes
\href{https://www.tensorflow.org}{TensorFlow},
\href{http://deeplearning.net/software/theano/}{Theano},
\href{http://torch.ch}{Torch},
\href{https://github.com/NervanaSystems/neon}{neon},
\href{http://rll.berkeley.edu/cgt/}{Computational Graph Toolkit}, and
\href{https://github.com/stan-dev/math}{Stan Math Library}. These are
incredible tools which Edward employs as a backend. In
terms of abstraction, Edward sits one level higher.

\textbf{High-level deep learning libraries.}
Neural network libraries such as
\href{https://github.com/fchollet/keras}{Keras} and
\href{https://github.com/Lasagne/Lasagne}{Lasagne} are at a similar
abstraction level as us, but they are primarily interested in
parameterizing complicated functions for supervised learning on large
datasets. We are interested in probabilistic models which apply
to a wide array of learning tasks, and which can have both
complicated likelihood and complicated priors (neural networks are an
option but not a necessity). Therefore our goals are orthogonal and in
fact mutually benefit each other. For example, we use Keras'
abstraction as a way to easily specify models parameterized by deep
neural networks.

\subsubsection{References}\label{references}
